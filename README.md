````markdown
# DSTÂ â€“ Ruleâ€‘Based Classification with RIPPERâ€¯/â€¯FOILâ€¯+â€¯Dempsterâ€‘Shafer Theory  

<p align="center">
  <img src="Common_code/benchmark_dataset6.png" width="650">
</p>

> â€¢ **Automatic rule induction** (RIPPERÂ /Â FOIL)  
> â€¢ **Dempsterâ€‘Shafer** evidence aggregation & mass optimisation  
> â€¢ **Postâ€‘pruning** that keeps the model tiny without hurting accuracy  

---

##Â 1.Â Quick start  

```bash
git clone https://github.com/SargisVardanian/DST.git
cd DST
python test_Ripper_DST.py        # full benchmark pipeline
````

`test_Ripper_DST.py` will

1. learn rules with **RIPPER**â€¯andâ€¯**FOIL**;
2. convert them to Dempsterâ€‘Shafer rules;
3. optimise masses via gradient descent, optionally prune;
4. save everything to portable `.dsb` files and plot the metrics you saw above.

---

\##Â 2.Â Project layout

```
core.py                   # helper maths (mass init, commonality, â€¦)
utils.py                  # misc helpers (IO, formatting, â€¦)
DSRule.py                 # tiny wrapper: predicateÂ +Â caption
DSRipper.py               # incremental RIPPER/FOIL inducer
DSModel.py                # singleâ€‘class DST model
DSModelMultiQ.py          # multiâ€‘class DST model
DSClassifier.py           # sklearnâ€‘style wrapper (binary)
DSClassifierMultiQ.py     # sklearnâ€‘style wrapper (multiâ€‘class)
Datasets_loader.py        # six toyÂ + real binary datasets
test_Ripper_DST.py        # endâ€‘toâ€‘end experiment script
```

---

\##Â 3.Â Rule induction with **DSRipper**

\###Â 3.1Â Algorithm in a nutshell

We follow the classical *RIPPER* loopÂ (CohenÂ 1995):

1. **Grow** â€“ add literals that maximise informationâ€‘gain

   <img style="background-color:#f7f7f7; padding:4px 8px; border-radius:4px;"
     src="https://latex.codecogs.com/svg.image?\mathrm{Gain}(r)%20=%20p_{\text{new}}\!\left(\log_2\frac{p_{\text{new}}}{p_{\text{new}}+n_{\text{new}}}-\log_2\frac{p_{\text{old}}}{p_{\text{old}}+n_{\text{old}}}\right)" />

2. **Prune** the rule with reducedâ€‘error pruning on a heldâ€‘out slice.

3. **Delete** covered positives; repeat until no positives remain.

`DSRipper` supports two modes

| `algo`      | Gain formula                                                                                                                                                     |
| ----------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `"ripper"`  | classic gain above                                                                                                                                               |
| `"foil"`Â Â Â  | <img style="background-color:#f7f7f7; padding:4px 8px; border-radius:4px;" src="https://latex.codecogs.com/svg.image?\mathrm{FOILGain}=p(\log_2 t'-\log_2 t)" /> |

We run induction **incrementally in miniâ€‘batches** â€“ lets 6â€¯kâ€‘row Wine dataset fit into RAM.

\###Â 3.2Â Output

For each classÂ *ğ‘* we get a list of dictâ€‘rules, e.g.

```python
{'free sulfur dioxide': ('<', 36),
 'total sulfur dioxide': ('>=', 110)}
```

Then we build `DSRule` objects:

```python
lam  = lambda x, thr=36: x[idx] < thr          # compiled predicate
rule = DSRule(ld=lam,
              caption="ClassÂ 1: freeÂ SOâ‚‚ <Â 36 & totalÂ SOâ‚‚ â‰¥Â 110")
```

At this point rule only knows **coverage**Â |{xÂ : rule(x)}|; quality metrics come later fromâ€¯DST.

---

\##Â 4.Â Dempsterâ€“Shafer model

\###Â 4.1Â Mass assignmentÂ (MAF)

Each ruleÂ *R*<sub>i</sub> carries a mass vector

<img style="background-color:#f7f7f7; padding:4px 8px; border-radius:4px;"
  src="https://latex.codecogs.com/svg.image?\mathbf{m}^{(i)}=(m_1^{(i)},\dots,m_K^{(i)},\,m_{\mathrm{unc}}^{(i)}),\quad\sum_{j=1}^K m_j^{(i)}+m_{\mathrm{unc}}^{(i)}=1" />

We initialise them **biased to uncertainty** (e.g.Â *m*<sub>unc</sub>=0.8) or by a smarter clusteringâ€‘based scheme (DSGD++).

\###Â 4.2Â CombinationÂ â†’ commonality space

For a sample **x** denote $\mathcal R(x)=\{i:R_i(x)=\text{True}\}$.

1. **Massâ€¯â†’â€¯commonality**

   <img style="background-color:#f7f7f7; padding:4px 8px; border-radius:4px;"
     src="https://latex.codecogs.com/svg.image?q_k^{(i)}=m_k^{(i)}+m_{\mathrm{unc}}^{(i)},\quad k=1..K" />

2. **Combine** by pointâ€‘wise product

   <img style="background-color:#f7f7f7; padding:4px 8px; border-radius:4px;"
     src="https://latex.codecogs.com/svg.image?q_k(x)=\prod_{i\in\mathcal{R}(x)}q_k^{(i)}" />

3. **Back to masses**

   <img style="background-color:#f7f7f7; padding:4px 8px; border-radius:4px;"
     src="https://latex.codecogs.com/svg.image?\hat{m}_k(x)=\frac{q_k(x)}{\sum_{\ell=1}^K q_\ell(x)},\quad\hat{m}_{\mathrm{unc}}(x)=0" />

Prediction = $\arg\max_k\hat m_k(x)$.

\###Â 4.3Â Mass optimisationÂ (DSTâ€‘GD)

Treat every $\mathbf m^{(i)}$ as a learnable tensor; minimise crossâ€‘entropy (or MSE) with Adam.
`DSModel*.normalize()` enforces $m\ge0$ and $\sum=1$.

---

\##Â 5.Â Rule quality & pruning

After optimisation each ruleÂ *R*<sub>i</sub> has

* uncertaintyÂ Â Â <img style="background-color:#f7f7f7; padding:4px 8px; border-radius:4px;" src="https://latex.codecogs.com/svg.image?u_i=m_{\mathrm{unc}}^{(i)}" />
* topâ€‘2 ratioÂ Â Â <img style="background-color:#f7f7f7; padding:4px 8px; border-radius:4px;" src="https://latex.codecogs.com/svg.image?r_i=\frac{\max_j m_j^{(i)}}{\text{2ndâ€‘largest }m^{(i)}+10^{-3}}" />

Scale $r_i\rightarrow r'_i\in[0,1]$; usefulness

<img style="background-color:#f7f7f7; padding:4px 8px; border-radius:4px;"
  src="https://latex.codecogs.com/svg.image?H_i=\frac{2(1-u_i)r'_i}{(1-u_i)+r'_i}" />

Coverage $c_i=|\{x:R_i(x)\}|$.Â We **drop** a rule when

* $u_i>0.7$ **and**
* $H_i<0.4$ **and**
* $c_i<10$

Wineâ€‘quality dataset: rulesÂ 42Â â†’Â 33, identicalâ€¯F1.

---

\##Â 6.Â Benchmarks

| Variant       |  Acc. |    F1 | Prec. |  Rec. |
| ------------- | ----: | ----: | ----: | ----: |
| **R\_raw**    | 0.786 | 0.766 | 1.000 | 0.621 |
| **R\_DST**    | 0.983 | 0.988 | 1.000 | 0.970 |
| **R\_pruned** | 0.983 | 0.985 | 1.000 | 0.970 |
| **F\_raw**    | 0.982 | 0.984 | 0.992 | 0.975 |
| **F\_DST**    | 0.989 | 0.990 | 0.992 | 0.989 |
| **F\_pruned** | 0.989 | 0.991 | 0.986 | 0.995 |

> **DST weighting** lifts plain RIPPER from mediocre to SOTA.
> **Pruning** cuts model size byÂ â‰ˆâ€¯20â€¯% with *no* accuracy loss.

---

\##Â 7.Â CitingÂ & further reading

* G.Â Shafer â€” *A Mathematical Theory of Evidence* (1976)
* W.Â Cohen â€” *Fast Effective Rule Induction* (ICMLÂ 1995)
* R.Â Quinlan â€” *FOIL:Â A Midterm Report* (1990)
* S.Â PeÃ±afielâ€¯etâ€¯al. â€” *Applying DST for an Interpretable Classifier* (ESWAÂ 2020)

